{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "b246a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import uuid\n",
    "\n",
    "\n",
    "class DeviceType(Enum):\n",
    "    CPU = 0\n",
    "    GPU = 1\n",
    "    \n",
    "class GPUSpec:\n",
    "    def __init__(self,  name, csv_name):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.name = name\n",
    "        self.csv_name = csv_name\n",
    "    \n",
    "class Device:\n",
    "    def __init__(self, name, device_type=DeviceType.GPU, GPUSpec=None, task=None, device_id=uuid.uuid4()):\n",
    "        self.name = name\n",
    "        self.GPUSpec = GPUSpec\n",
    "        self.type = device_type\n",
    "        self.task = task\n",
    "        self.remain_time = 0\n",
    "    \n",
    "    def assign(self, task):\n",
    "        print(f\"[Log] Assign layer {task.current_layer} of task {task.model.name} to device {self.name}\")\n",
    "        model = task.model\n",
    "        layer_latency = model.layer_latency[1][self.GPUSpec.id][task.current_layer]\n",
    "        self.remain_time = layer_latency\n",
    "        self.task = task\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, name, layer_input_shape, layer_latency):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.name = name\n",
    "        self.layer_input_shape = layer_input_shape\n",
    "        self.layer_latency = layer_latency\n",
    "        self.layer_movement_time = self.get_movement_time(self.layer_input_shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movement_time(layer_input_shape):\n",
    "        movement_time = list()\n",
    "\n",
    "        for shape in layer_input_shape:\n",
    "            sleep(0.1)\n",
    "            data = torch.randn(shape)\n",
    "            data.to(0)\n",
    "            with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "                data.to(1)\n",
    "            movement_time.append(round(prof.self_cpu_time_total * 1000))\n",
    "        movement_time.append(0)\n",
    "        return movement_time\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, model, current_layer=0, input_data_position=0):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.model = model\n",
    "        self.current_layer = current_layer\n",
    "        self.input_date_position = input_data_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "65150f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_2060 = GPUSpec(\"RTX 2060\", \"2060\")\n",
    "GPU_1080 = GPUSpec(\"GTX 1008 Ti\", \"1080\")\n",
    "gpu_list = (GPU_2060, GPU_1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "c714ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from torchvision.models import resnet18, vgg16, alexnet, mobilenet_v3_large\n",
    "\n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child\n",
    "        return model\n",
    "    else:\n",
    "        # look for children from children... to the last child\n",
    "        for child in children:\n",
    "            try:\n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "def get_all_shape(model):\n",
    "    children = get_children(model)\n",
    "    all_input_shape = list()\n",
    "\n",
    "    def make_forward(original_forward):\n",
    "        def new_forward(x):\n",
    "            all_input_shape.append(x.shape)\n",
    "            out = original_forward(x)\n",
    "            return out\n",
    "        return new_forward\n",
    "\n",
    "    for layer in children:\n",
    "        original_forward = layer.forward\n",
    "        layer.forward = make_forward(original_forward)\n",
    "\n",
    "    data = torch.randn(1, 3, 224, 224)\n",
    "    result = model(data)\n",
    "    return all_input_shape\n",
    "\n",
    "\n",
    "\n",
    "max_batch = 3\n",
    "\n",
    "def prune_df(df):\n",
    "    trash_list = list()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['Op'] in ['__add__', '__iadd__', '__mul__']:\n",
    "            trash_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    for i in trash_list:\n",
    "        df.at[i-1, 'duration'] = df.iloc[i]['duration'] + df.iloc[i-1]['duration']\n",
    "    return df.drop(trash_list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "dc79a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_layer = dict()\n",
    "for i in range(1, max_batch + 1):\n",
    "    for gpu in gpu_list:\n",
    "        filename = f\"Resnet/ResNet_{gpu.csv_name}_{i}.csv\"\n",
    "        if not i in resnet_layer:\n",
    "            resnet_layer[i] = dict()\n",
    "        resnet_layer[i][gpu.id] = prune_df(pd.read_csv(filename))['duration']\n",
    "\n",
    "ResNet = Model(\"ResNet\", get_all_shape(resnet18()), resnet_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "45b64cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_layer = dict()\n",
    "for i in range(1, max_batch + 1):\n",
    "    for gpu in gpu_list:\n",
    "        filename = f\"AlexNet/ResNet_{gpu.csv_name}_{i}.csv\"\n",
    "        if not i in alexnet_layer:\n",
    "            alexnet_layer[i] = dict()\n",
    "        alexnet_layer[i][gpu.id] = prune_df(pd.read_csv(filename))['duration']\n",
    "        \n",
    "AlexNet = Model(\"AlexNet\", get_all_shape(alexnet()), alexnet_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "f472c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = dict()\n",
    "for i in range(1, max_batch + 1):\n",
    "    for gpu in gpu_list:\n",
    "        filename = f\"rnn_result/rnn_{gpu.csv_name}_{i}.csv\"\n",
    "        if not i in rnn_layer:\n",
    "            rnn_layer[i] = dict()\n",
    "        rnn_layer[i][gpu.id] = prune_df(pd.read_csv(filename))['duration']\n",
    "        \n",
    "rnn_shape = [torch.Size([1, 28, 28]), torch.Size([1, 28, 28]), torch.Size([1, 128])]\n",
    "RNN = Model(\"RNN\", rnn_shape, rnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "dd164380",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = [ResNet, AlexNet, RNN]\n",
    "Tasks = list()\n",
    "Devices = list()\n",
    "Devices.append(Device(\"GPU\", device_type=DeviceType.CPU, device_id=0))\n",
    "\n",
    "GPU_0 = Device(\"GPU (2060)\", device_type=DeviceType.GPU, GPUSpec=GPU_2060)\n",
    "GPU_1 = Device(\"GPU (1080)\", device_type=DeviceType.GPU, GPUSpec=GPU_1080)\n",
    "Devices.append(GPU_0)\n",
    "Devices.append(GPU_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "761c22b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet 2705521\n",
      "AlexNet 1463499\n",
      "RNN 459672\n"
     ]
    }
   ],
   "source": [
    "for model in Models:\n",
    "    print(model.name, sum(model.layer_latency[1][GPU_2060.id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "1fecdb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task_1 = Task(ResNet)\n",
    "Task_2 = Task(AlexNet)\n",
    "Task_3 = Task(ResNet)\n",
    "Task_4 = Task(AlexNet)\n",
    "Tasks.append(Task_1)\n",
    "Tasks.append(Task_2)\n",
    "Tasks.append(Task_3)\n",
    "Tasks.append(Task_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "cfa95a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tasks = [Task(ResNet), Task(ResNet), Task(AlexNet), Task(AlexNet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b1d7cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Assign layer 0 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 0 of task AlexNet to device GPU (1080)\n"
     ]
    }
   ],
   "source": [
    "GPU_0.assign(Task_1)\n",
    "GPU_1.assign(Task_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c7196542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Assign layer 0 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 0 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 1 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 1 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 2 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 3 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 4 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 5 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 6 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 7 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 2 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 8 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 3 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 9 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 10 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 4 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 11 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 12 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 13 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 5 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 14 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 6 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 7 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 15 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 16 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 8 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 9 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 10 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 17 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 18 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 19 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 11 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 12 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 13 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 20 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 21 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 14 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 22 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 15 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 16 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 23 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 24 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 25 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 26 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 27 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 17 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 18 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 19 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 28 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 29 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 30 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 20 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 21 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 22 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 23 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 24 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 31 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 32 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 33 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 25 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 26 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 27 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 34 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 35 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 28 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 36 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 29 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 30 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 37 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 38 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 39 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 40 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 41 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 31 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 32 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 33 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 42 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 43 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 44 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 34 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 35 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 36 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 37 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 38 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 39 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 40 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 41 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 45 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 46 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 47 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 42 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 43 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 44 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 48 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 49 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 50 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 51 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 52 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 45 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 46 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 47 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 53 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 54 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 55 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 48 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 49 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 56 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 57 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 58 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 50 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 59 of task ResNet to device GPU (1080)\n",
      "[Log] Assign layer 51 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 52 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 53 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 54 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 55 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 56 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 57 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 58 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 59 of task ResNet to device GPU (2060)\n",
      "[Log] Assign layer 0 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 0 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 1 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 2 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 1 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 2 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 3 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 3 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 4 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 5 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 4 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 5 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 6 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 6 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 7 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 8 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 7 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 8 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 9 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 10 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 9 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 10 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 11 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 12 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 13 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 14 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 15 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 11 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 12 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 13 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 14 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 15 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 16 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 17 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 18 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 16 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 17 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 18 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 19 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 20 of task AlexNet to device GPU (1080)\n",
      "[Log] Assign layer 19 of task AlexNet to device GPU (2060)\n",
      "[Log] Assign layer 20 of task AlexNet to device GPU (2060)\n",
      "[Result] Tail latency: 4169020\n"
     ]
    }
   ],
   "source": [
    "tail_latency = 0\n",
    "while True:\n",
    "    min_delay = float(\"inf\")\n",
    "    for device in Devices:\n",
    "        if device.type == DeviceType.GPU:\n",
    "            if device.task:\n",
    "                min_delay = min((min_delay, device.remain_time))\n",
    "    \n",
    "    for device in Devices:\n",
    "        if device.type == DeviceType.GPU:\n",
    "            if device.task:\n",
    "                device.remain_time -= min_delay\n",
    "                if device.remain_time == 0:\n",
    "                    if device.task.current_layer + 1 < len(device.task.model.layer_input_shape):\n",
    "                        device.task.current_layer += 1\n",
    "                        device.assign(device.task)\n",
    "                    else:\n",
    "                        device.task = None\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    \n",
    "    if min_delay == float(\"inf\"):\n",
    "        if len(Tasks) == 0:\n",
    "            break\n",
    "        else:\n",
    "            for device in Devices:\n",
    "                if device.type == DeviceType.GPU:\n",
    "                    if not device.task and len(Tasks):\n",
    "                        device.assign(Tasks.pop(0))\n",
    "    else:\n",
    "        tail_latency += min_delay\n",
    "print(f\"[Result] Tail latency: {tail_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "417226d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705521\n",
      "2100057\n",
      "1463499\n",
      "1302990\n"
     ]
    }
   ],
   "source": [
    "for i in ResNet.layer_latency[1].items():\n",
    "    print(sum(i[1]))\n",
    "for i in AlexNet.layer_latency[1].items():\n",
    "    print(sum(i[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
